{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Create a directory for saving checkpoints\n",
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "class ArcDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        if self.transform:\n",
    "            # TODO: Do we want to transform the original? If so, how do we know that both should still be close in embedding space?\n",
    "            # img1 = self.transform(img)\n",
    "            img1 = img\n",
    "            img2 = self.transform(img)\n",
    "        else:\n",
    "            img1, img2 = img, img\n",
    "        return img1, img2  # Return twice for SimCLR\n",
    "\n",
    "# Transformer Encoder\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, img_size, d_model, num_layers, num_heads, ff_dim, dropout, use_positional_encoding=True):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.patch_size = 1  # Each pixel is a patch\n",
    "        self.num_patches = img_size * img_size\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.embedding = nn.Linear(self.patch_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.use_positional_encoding = use_positional_encoding\n",
    "        if self.use_positional_encoding:\n",
    "            self.positional_encoding = nn.Parameter(torch.zeros(1, self.num_patches, d_model))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=ff_dim, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten image into patches\n",
    "        x = x.view(x.size(0), -1, self.patch_size)  # (batch_size, num_patches, patch_size)\n",
    "        \n",
    "        # Linear embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Add positional encoding if enabled\n",
    "        if self.use_positional_encoding:\n",
    "            x += self.positional_encoding\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = torch.mean(x, dim=1)\n",
    "        return x\n",
    "\n",
    "# Define the projection head\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        inter_dim = 128\n",
    "        self.fc1 = nn.Linear(input_dim, inter_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(inter_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the SimCLR model with transformer encoder\n",
    "class SimCLR(nn.Module):\n",
    "    def __init__(self, transformer_encoder, projection_dim):\n",
    "        super(SimCLR, self).__init__()\n",
    "        self.encoder = transformer_encoder\n",
    "        self.projection_head = ProjectionHead(transformer_encoder.embedding.out_features, projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.projection_head(x)\n",
    "        return x\n",
    "\n",
    "# NT-Xent Loss remains the same\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        N = 2 * self.batch_size\n",
    "        z = torch.cat((z_i, z_j), dim=0)  # Concatenate along batch dimension\n",
    "        \n",
    "        # Compute cosine similarity matrix\n",
    "        sim_matrix = torch.mm(z, z.t().contiguous()) / self.temperature\n",
    "\n",
    "        # Mask the diagonal (self-similarities) by setting them to a large negative value\n",
    "        mask = torch.eye(N, device=sim_matrix.device).bool()\n",
    "        sim_matrix.masked_fill_(mask, -1e9)\n",
    "\n",
    "        # Labels for positive pairs (originally batch_size, then indices shift by batch_size)\n",
    "        labels = torch.cat([torch.arange(self.batch_size, device=z.device) + self.batch_size * i for i in range(2)])\n",
    "\n",
    "        # Compute loss using the similarity matrix and the positive pair indices\n",
    "        loss = self.criterion(sim_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "# TODO: Replace with relevant ARC-AGI augmentations\n",
    "augmentation = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=30, scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, loss, checkpoint_dir='checkpoints'):\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch}.pth')\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss\n",
    "    }, checkpoint_path)\n",
    "    logger.info(f\"Checkpoint saved at {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 100\n",
    "CHECKPOINT_INTERVAL = 10\n",
    "\n",
    "images = None  # TODO: load images here\n",
    "dataset = ArcDataset(images, transform=augmentation)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer with transformer encoder\n",
    "transformer_encoder = TransformerEncoder(\n",
    "    img_size=30,\n",
    "    d_model=128,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    ff_dim=256,\n",
    "    dropout=0.1,\n",
    "    use_positional_encoding=True\n",
    ").to(device)\n",
    "\n",
    "model = SimCLR(transformer_encoder, projection_dim=128).to(device)\n",
    "criterion = NTXentLoss(batch_size=BATCH_SIZE, temperature=0.5)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003) # TODO: experiment w/ HPs like weight decay\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Training loop remains the same\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for img1, img2 in dataloader:\n",
    "        img1, img2 = img1.to(device), img2.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z_i = model(img1)\n",
    "        z_j = model(img2)\n",
    "        \n",
    "        loss = criterion(z_i, z_j)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # TODO: Experiment with scheduler\n",
    "    # scheduler.step()\n",
    "\n",
    "    if (epoch + 1) % CHECKPOINT_INTERVAL == 0:\n",
    "        save_checkpoint(epoch + 1, model, optimizer, avg_loss)\n",
    "        \n",
    "    logger.info(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "logger.info(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
